import os
import yaml
from pathlib import Path
from typing import Dict, Any, List, Optional
from PIL import Image

import chromadb
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from .embedder import MultimodalEmbedder

class MultimodalRAGChain:
    """
    A custom chain for performing multimodal RAG.
    This chain handles text and image inputs, retrieves relevant documents from a 
    multimodal ChromaDB, and generates a response using a language model.
    """
    def __init__(self, config_path: Optional[str] = None):
        """
        Initializes the RAG chain components.
        
        Args:
            config_path (str, optional): Path to the YAML config file. 
                                         Defaults to 'rag_config.yaml' in the same directory.
        """
        if config_path is None:
            config_path = str(Path(__file__).parent / "rag_config.yaml")
        
        print("--- Initializing Multimodal RAG Chain ---")
        
        # 1. Load Configuration
        with open(config_path, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)
        
        llm_cfg = config["model_config"]["llm_model"]
        vs_cfg = config["vector_store_config"]
        prompt_cfg = config["model_config"]["prompt_template"]

        # 2. Initialize Language Model
        # æ„å›¾è¯†åˆ«ç”¨å°æ¨¡å‹
        self.intent_llm = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-lite",
            temperature=0.1,  # ä½æ¸©åº¦ç¡®ä¿ä¸€è‡´æ€§
            max_output_tokens=100,
            google_api_key=os.getenv("GOOGLE_API_KEY")
        )

        self.llm = ChatGoogleGenerativeAI(
            model=llm_cfg["name"],
            temperature=llm_cfg["temperature"],
            max_output_tokens=llm_cfg["max_length"],
            google_api_key=os.getenv("GOOGLE_API_KEY")
        )
        print(f"LLM initialized: {llm_cfg['name']}, intent_llm initialized: gemini-2.0-flash-lite")

        # 3. Initialize Multimodal Embedder
        self.embedder = MultimodalEmbedder()
        self.embedding_function = self.embedder.get_embedding_function()
        print("Multimodal embedder initialized.")

        # 4. Initialize ChromaDB Clients
        case_cfg = vs_cfg["case_level"]
        chunk_cfg = vs_cfg["chunk_level"]
        
        # Caseçº§åˆ«æ•°æ®åº“
        case_db_path = case_cfg["persist_directory"]
        self.case_client = chromadb.PersistentClient(path=case_db_path)
        self.case_collection = self.case_client.get_collection(name=case_cfg["collection_name"])
        print(f"Case-level database connected: {case_db_path}")
        
        # Chunkçº§åˆ«æ•°æ®åº“
        chunk_db_path = chunk_cfg["persist_directory"]
        self.chunk_client = chromadb.PersistentClient(path=chunk_db_path)
        self.chunk_collection = self.chunk_client.get_collection(name=chunk_cfg["collection_name"])
        print(f"Chunk-level database connected: {chunk_db_path}")
        
        # 5. Initialize settings
        self.case_k = case_cfg["search_kwargs"]["k"]
        self.chunk_k = chunk_cfg["search_kwargs"]["k"]
        self.prompt = PromptTemplate(
            template=prompt_cfg["template"],
            input_variables=prompt_cfg["input_variables"]
        )
        print("Two-Stage RAG chain components are ready.")

        # 6. æ„å›¾è¯†åˆ«prompt
        self.intent_prompt = PromptTemplate(
            template="""åˆ†æç”¨æˆ·æŸ¥è¯¢æ„å›¾ï¼Œè¿”å›æ£€ç´¢ç­–ç•¥ã€‚

        æ–‡æœ¬: {query_text}
        å›¾ç‰‡: {has_image}

        è§„åˆ™:
        - æ–‡æœ¬æ˜¯é€šç”¨æŒ‡ä»¤("æ£€ç´¢å›¾ç‰‡ä¿¡æ¯"ç­‰)ä¸”æœ‰å›¾ç‰‡ â†’ image_only
        - æ–‡æœ¬æ˜¯å…·ä½“åŒ»å­¦é—®é¢˜ä¸”æœ‰å›¾ç‰‡ â†’ mixed_query  
        - åªæœ‰æ–‡æœ¬ â†’ text_only
        - åªæœ‰å›¾ç‰‡ â†’ image_only

        è¿”å›: image_only / text_only / mixed_query""",
                    input_variables=["query_text", "has_image"]
        )

    def invoke(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """
        Executes the two-stage RAG chain.
        """
        query_text = inputs.get("input")
        query_image = inputs.get("image")

        if not query_text and not query_image:
            raise ValueError("Query must contain at least 'input' text or an 'image'.")

        # LLMæ„å›¾è¯†åˆ«
        intent_chain = self.intent_prompt | self.intent_llm
        response = intent_chain.invoke({
            "query_text": query_text if query_text else "",
            "has_image": "æ˜¯" if query_image else "å¦"
        })
        
        intent = response.content.strip().lower()
        print(f"ğŸ¯ æ„å›¾è¯†åˆ«ç»“æœ: {intent}")
        
        # æ ¹æ®æ„å›¾è°ƒæ•´æŸ¥è¯¢
        if intent == "image_only":
            print(f"ğŸ–¼ï¸ æ„å›¾è¯†åˆ«: çº¯å›¾ç‰‡æ£€ç´¢")
            query_text = None
        elif intent == "text_only":
            print(f"ğŸ“ æ„å›¾è¯†åˆ«: çº¯æ–‡æœ¬æ£€ç´¢")
            query_image = None
        elif intent == "mixed_query":
            print(f"ğŸ”„ æ„å›¾è¯†åˆ«: æ··åˆæŸ¥è¯¢")
        else:
            print(f"âš ï¸ æœªçŸ¥æ„å›¾ï¼Œä½¿ç”¨æ··åˆæŸ¥è¯¢")
            intent = "mixed_query"
            
        # Stage 1: Case-levelæ£€ç´¢
        print("ğŸ” Stage 1: Case-levelæ£€ç´¢")
        case_results = self._case_level_retrieval(query_text, query_image)
        
        # Stage 2: Chunk-levelæ£€ç´¢
        print("ğŸ” Stage 2: Chunk-levelæ£€ç´¢")
        chunk_results = self._chunk_level_retrieval(query_text, query_image, case_results)
        
        # ç”Ÿæˆç­”æ¡ˆ
        context_docs = chunk_results['documents']

        print(f"Contextæ•°é‡: {len(context_docs)}")
        print(f"Contextå†…å®¹: {context_docs[:2]}...")  # æ˜¾ç¤ºå‰ä¸¤ä¸ªæ–‡æ¡£

        context = "\n\n---\n\n".join(context_docs)
        print(f"åˆå¹¶åContexté•¿åº¦: {len(context)}")

        chain = self.prompt | self.llm
        response = chain.invoke({
            "context": context,
            "input": query_text if query_text else ""
        })

        # æå–å›¾ç‰‡è·¯å¾„
        image_paths = []
        for metadata_list in case_results['metadatas']:
            for metadata in metadata_list:
                if metadata and 'image_path' in metadata and metadata['image_path']:
                    image_paths.append(metadata['image_path'])

        # è°ƒè¯•ä¿¡æ¯
        try:
            self._print_debug_info_two_stage(query_text, query_image, case_results, chunk_results)
        except Exception as e:
            print(f"âš ï¸ è°ƒè¯•ä¿¡æ¯æ‰“å°å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()  # æ‰“å°å®Œæ•´é”™è¯¯å †æ ˆ
        
        return {
            "context": context,
            "answer": response.content,
            "image_paths": image_paths,
            "case_results": case_results,
            "chunk_results": chunk_results
        }

    def _case_level_retrieval(self, query_text, query_image):
        """Caseçº§åˆ«æ£€ç´¢"""
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = self.embedding_function(texts=[query_text], images=[query_image])
        
        # åœ¨caseçº§åˆ«æ•°æ®åº“ä¸­æ£€ç´¢
        case_results = self.case_collection.query(
            query_embeddings=query_embedding,
            n_results=self.case_k,
            include=['documents', 'metadatas', 'distances']
        )
        
        return case_results

    def _chunk_level_retrieval(self, query_text, query_image, case_results):
        """Chunkçº§åˆ«æ£€ç´¢"""
        # åªè·å–ç›¸ä¼¼åº¦æœ€é«˜çš„caseçš„ID
        if not case_results['metadatas'][0]:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³caseï¼Œè¿”å›ç©ºç»“æœ")
            return {'documents': [], 'metadatas': [], 'distances': []}
        
        # åªå–ç¬¬ä¸€ä¸ªï¼ˆç›¸ä¼¼åº¦æœ€é«˜çš„ï¼‰case
        top_case_metadata = case_results['metadatas'][0][0]
        top_case_id = top_case_metadata.get('case_id')
        
        if not top_case_id:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³caseï¼Œè¿”å›ç©ºç»“æœ")
            return {'documents': [], 'metadatas': [], 'distances': []}
        
        print(f"ğŸ¯ åœ¨ç›¸ä¼¼åº¦æœ€é«˜çš„caseä¸­æ£€ç´¢: {top_case_id}")

        # å¦‚æœquery_textä¸ºNoneï¼Œä½¿ç”¨é»˜è®¤æ–‡æœ¬
        if query_text is None:
            query_text = "image analysis"
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼ˆchunkçº§åˆ«ä¸ä½¿ç”¨å›¾ç‰‡ï¼‰
        query_embedding = self.embedding_function(texts=[query_text], images=[None])
        
        # åªåœ¨ç›¸ä¼¼åº¦æœ€é«˜çš„caseçš„chunksä¸­æ£€ç´¢
        chunk_results = self.chunk_collection.query(
            query_embeddings=query_embedding,
            n_results=self.chunk_k,
            include=['documents', 'metadatas', 'distances'],
            where={"case_id": top_case_id}  # åªæ£€ç´¢ä¸€ä¸ªcase
        )
        
        # åœ¨è¿”å›å‰å±•å¹³ç»“æœï¼ˆå¦‚æœéœ€è¦ï¼‰
        if isinstance(chunk_results['documents'], list) and len(chunk_results['documents']) > 0:
            if isinstance(chunk_results['documents'][0], list):
                # åµŒå¥—ç»“æ„ï¼Œå±•å¹³
                chunk_results['documents'] = [doc for doc_list in chunk_results['documents'] for doc in doc_list]
                chunk_results['metadatas'] = [meta for meta_list in chunk_results['metadatas'] for meta in meta_list]
                chunk_results['distances'] = [dist for dist_list in chunk_results['distances'] for dist in dist_list]
        
        return chunk_results

    def _print_debug_info_two_stage(self, query_text, query_image, case_results, chunk_results):
        """æ˜¾ç¤ºä¸¤é˜¶æ®µæ£€ç´¢çš„è°ƒè¯•ä¿¡æ¯"""
        print(f"\n{'='*50}")
        print(f"ğŸ” ä¸¤é˜¶æ®µRAGæ£€ç´¢è°ƒè¯•ä¿¡æ¯")
        print(f"{'='*50}")
        
        # æŸ¥è¯¢ä¿¡æ¯
        print(f" æ–‡æœ¬æŸ¥è¯¢: '{query_text}'")
        print(f"ğŸ–¼ï¸ å›¾ç‰‡æŸ¥è¯¢: {'æœ‰' if query_image else 'æ— '}")
        
        # Stage 1: Caseçº§åˆ«ç»“æœ
        print(f"\n--- Stage 1: Caseçº§åˆ«æ£€ç´¢ç»“æœ ---")
        for i, (distance, metadata, document) in enumerate(zip(
            case_results['distances'][0],
            case_results['metadatas'][0],
            case_results['documents'][0]
        )):
            similarity = 1.0 / (1.0 + distance)
            print(f"ğŸ“¸ Case {i+1}:")
            print(f"    ğŸ“ è·ç¦»: {distance:.4f}")
            print(f"    ç›¸ä¼¼åº¦: {similarity:.4f} ({similarity*100:.1f}%)")
            print(f"    å…ƒæ•°æ®: {metadata}")
            print(f"    ğŸ“ å†…å®¹é¢„è§ˆ: {document[:100]}...")
            print()
        
        # Stage 2: Chunkçº§åˆ«ç»“æœ
        print(f"\n--- Stage 2: Chunkçº§åˆ«æ£€ç´¢ç»“æœ ---")
        for i, (distance, metadata, document) in enumerate(zip(
            chunk_results['distances'],
            chunk_results['metadatas'],
            chunk_results['documents']
        )):
            similarity = 1.0 / (1.0 + distance)
            print(f"ğŸ“„ Chunk {i+1}:")
            print(f"    ğŸ“ è·ç¦»: {distance:.4f}")
            print(f"    ç›¸ä¼¼åº¦: {similarity:.4f} ({similarity*100:.1f}%)")
            print(f"    å…ƒæ•°æ®: {metadata}")
            print(f"    ğŸ“ å†…å®¹é¢„è§ˆ: {document[:100]}...")
            print()
        
        # ç»Ÿè®¡ä¿¡æ¯
        print(f"ğŸ“Š æ£€ç´¢ç»Ÿè®¡:")
        print(f"   ğŸ“¸ Caseçº§åˆ«æ£€ç´¢: {len(case_results['documents'][0])}ä¸ªcase")
        print(f"   Chunkçº§åˆ«æ£€ç´¢: {len(chunk_results['documents'])}ä¸ªchunk")
        print(f"{'='*50}")