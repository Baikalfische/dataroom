<div align="center">

# ğŸ  Real Estate Dataroom

A minimal but working intelligent Q&A prototype for real estate / contract documents: upload PDF / CSV, invoke a single RAG retrieval tool, generate answers with citations via Google Gemini. Focus is on architecture & design tradeâ€‘offs, not piling on features.

</div>

## âœ¨ Current Functionality

This version is a â€œminimum auditableâ€ RAG skeleton for real estate / contract docs: multiple PDF / CSV uploads, automatic parse â†’ chunk â†’ embed â†’ persist; a single retrieval tool is called by the Agent; answers include traceable citations. Goal: keep the full loop (Ingest â†’ Retrieve â†’ Assemble â†’ Answer with Citations) stable & demoâ€‘able, deferring heavy optimization.

Implemented highlights (only what matters for future improvements):
- Ingestion: multiple uploads; PDF by page, CSV by row (citations map directly to physical page/row).
- Separate stores: PDF / CSV in distinct Chroma collections to avoid crossâ€‘modality noise; enables later perâ€‘type k / filters.
- Embeddings: CLIP 512d (fast + cached) baseline before swapping to domain models or adding rerank.
- Retrieval pipeline: independent TopK per store then merge; debug hooks allow recall inspection.
- Agent loop: LangGraph singleâ€‘tool cycle; system prompt enforces â€œretrieve when unsure / no fabrication.â€
- Citations: `[filename p.X]` / `[filename row Y]`; duplicates allowed (later can merge).
- File listing: button shows current documents & page/row counts.
- Local run: single process, no external vector infra.

> These foundations act as the â€œslabâ€; later precision tactics (Hybrid, rerank, finer splitting, structured index) can layer on without API breaks.

## ğŸš§ Pain Points to Improve
- Repetitive clause templates (e.g. rent escalation / liability) blur embeddings.
- Long multiâ€‘topic pages introduce noise when recalled wholesale.
- Precise numeric / date alignment fragile (minor phrasing variance).
- Crossâ€‘document comparisons (e.g. highest escalation) need structured extraction & aggregation.
- Clause traceability limited (page/row only; lacks clause numbering / headings).
- Vague queries (â€œearly termination?â€) need rewrite / subâ€‘query expansion.
- No labeled eval set yet â†’ tuning lacks objective feedback.

## ğŸ› ï¸ Retrieval Accuracy Strategies

Startupâ€‘pragmatic two tiers: first â€œstructure + coverageâ€, then â€œgranularity + rerankâ€.

1. Lightweight (fast, no core refactor)
   - Hybrid: vector + simple inverted (BM25 / keyword scoring) to patch pure semantic misses.
   - Clause header signal: parse â€œSection 3.2 / Article Xâ€ into metadata to boost direct hits.
   - Query expansion: LLM synonyms / field subâ€‘queries (amount, date) â†’ parallel retrieve + dedupe merge.
   - Candidate dedup: similarity cluster initial TopK to reduce nearâ€‘duplicate clauses.
   - Adaptive TopK: numeric / clauseâ€‘pattern queries increase structural weighting.

2. Mid Layer (adds small components)
   - Secondary splitting: page â†’ sentence / clause; row â†’ field tokens; retrieve fine units then aggregate upward.
   - Semantic rerank: crossâ€‘encoder reorders ~30 firstâ€‘stage candidates.
   - Structured field extraction: regex + light model for amounts / dates / percentages (aux key/value index).
   - Multiâ€‘stage retrieval: keyword / clause preâ€‘filter â†’ vector expansion â†’ rerank.
   - Hard example logging: store â€œno answer / irrelevantâ€ feedback for retraining / reweighting.

## ğŸ§± Architecture Overview

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                             User / Browser                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      â”‚ question / upload (PDF, CSV)
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      Single turn flow (messageâ†’toolâ†’message)
â”‚         Gradio UI         â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  - File upload (FS)       â”‚                                      â”‚
â”‚  - Chat history           â”‚â—€â”€ answer + citations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚ invoke
      â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   single tool bound + system prompt
   â”‚  Agent (LangGraph)â”‚
   â”‚ - process node    â”‚
   â”‚ - execute node    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚ tool call: real_estate_document_search(query)
        â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚           RAG Chain          â”‚
   â”‚ 1. textâ†’vector (CLIP)        â”‚
   â”‚ 2. PDF vector search (k_pdf) â”‚
   â”‚ 3. CSV vector search (k_csv) â”‚
   â”‚ 4. merge / debug             â”‚
   â”‚ 5. LLM answer + citations    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚          â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”   â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ PDF Collectionâ”‚ â”‚ CSV Collectionâ”‚ (Chroma persisted)
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ main.py                     # entrypoint: init agent + launch UI
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ system_prompt.txt       # system prompt
â”œâ”€â”€ src/dataroom/agent/agent.py # agent (LangGraph state machine + tool exec)
â”œâ”€â”€ src/dataroom/rag/
â”‚   â”œâ”€â”€ build_database.py       # vector store mgmt
â”‚   â”œâ”€â”€ rag_chain.py            # retrieval + context assembly
â”‚   â”œâ”€â”€ embedder.py             # CLIP embeddings
â”‚   â”œâ”€â”€ chunks.py               # CSV row-level chunking
â”‚   â”œâ”€â”€ document_manager.py     # (upload wrapper, can be trimmed)
â”‚   â””â”€â”€ rag_config.yaml         # model / retrieval config
â”œâ”€â”€ src/dataroom/tools/rag_tool.py  # RAG tool
â”œâ”€â”€ src/dataroom/tools/parser.py    # PDF/CSV parsing
â”œâ”€â”€ src/dataroom/ui/interface.py    # minimal Gradio UI
â”œâ”€â”€ src/dataroom/utils/utils.py     # helpers
â”œâ”€â”€ chroma_db/ pdf_db/ csv_db/      # persistent vector DBs
â”œâ”€â”€ data/                           # sample docs
â””â”€â”€ tests/ test_parser.py           # basic test
```

## âš™ï¸ Quick Start

```bash
git clone <your-repo-url>
cd dataroom
python -m venv venv && source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -e .
export GOOGLE_API_KEY=YOUR_KEY   # Windows: set GOOGLE_API_KEY=...
python main.py
# visit http://127.0.0.1:7860
```

## ğŸ’¬ Usage
1. Upload PDF or CSV (repeat to build corpus).
2. Click â€œList Current Filesâ€ to view ingested documents and statistics.
3. Ask focused questions (include fields / assets / amounts).
4. Citations appear at the end; if unsure the system states uncertainty.

### Example Queries
```
â€œDoes the contract contain a rent escalation clause?â€
â€œWhich assets in the CSV have the highest NOI?â€
â€œWhat lease term (start/end) is mentioned on page 10 of the first PDF?â€
```

## ğŸ§  Key Design Decisions & Rationale

| Decision | Chosen | Main (Real) Motivation | Alternatives | Why Not Chosen | Future Direction |
|----------|--------|------------------------|--------------|----------------|------------------|
| Tool count | Single tool `real_estate_document_search` | Validate retrieval+citation loop only | Multiple tools (search/calc/format) | More orchestration complexity | Add structured calc / batch analysis tools later |
| Retrieval paradigm | Pure vector TopK (CLIP) | Lowest effort to prove viability | Hybrid (Vector+BM25) / keyword filter | Extra index & tuning | Introduce BM25 or custom inverted â†’ score fusion |
| Store split | Separate PDF / CSV collections | Avoid noise; clearer stats | Single collection + type field | Filtering still splits; stats less direct | Add unified aggregation view |
| Chunk granularity | PDF=page / CSV=row | Stable citations; quick | Sentence / sliding / semantic | Higher cleanup & complexity | Add secondary sentence split for long pages |
| Embedding model | CLIP 512d | Existing dep; good for MVP | bge / jina / text-embedding-004 | Time to evaluate & swap | Replace with domain text model + rerank |
| Context assembly | Simple concat + separators | Fast prototype; readable | Structured tables / clustering | Higher time cost | Field grouped summaries (rent/area) |
| Citation format | `[file p.X]` / `[file row Y]` | Short & clear | Inline JSON / hyperlinks | Verbose or UI work | Merge duplicate refs + anchors |
| Agent framework | LangGraph loop | Explicit states (respond vs tool) | Direct single-shot LLM | Harder to extend | Add rerank / filter nodes |
| UI | Single page upload + chat | Reduce UI surface; focus validation | Multi-tab / session mgmt | Out of scope | Add session save & citation replay |

> Reason for deferring Hybrid / rerank / secondary splitting now: 
> (1) Need more real doc diversity (contracts, statutes, rent schedules, ops reports) to profile template variance; 
> (2) Need a minimal evaluation set (20~50 labeled Q/A) to avoid blind tuning; 
> (3) Multiple strategies together require abstraction of a retrieval strategy layer & fusion weighting, adding shortâ€‘term complexity. **Hence: lock a working endâ€‘toâ€‘end baseline first, then iterate.**

## ğŸ§— Key Challenges & Reflections

| Challenge | Current Handling | Pain / Impact | Next Idea | Priority |
|-----------|-----------------|---------------|-----------|----------|
| Low accuracy (synonyms / long pages) | Basic vector TopK; page granularity | Missed recall / noise pages | Hybrid + sentence split + local rerank | High |
| Repetition & templated clauses | None yet | Subtle differences lost | Clause number regex + weighted fields | High |
| Single tool extensibility | All needs in one RAG | Prompt inflation later | Split tools (retrieval / calc / summary) | Medium |
| Coarse granularity | Quick build | Hard fine location; citation coarse | Secondary split (pageâ†’sentence; rowâ†’fields) | Medium |
| No rerank | None | Unstable TopK order | Add cross-encoder / miniLM reranker | Medium |
| Embedding generality | CLIP used | Domain nuance weak | Evaluate domain embeddings + cache migration | Medium |
| File listing scalability | Full scan get() | O(N) latency at scale | Maintain document_manifest index | Low |

> Current main bottleneck: lack of hybrid retrieval + finer-grain context leads to weaker performance on â€œsemantic + precise fieldâ€ questions. **Value delivered: traceable citations & basic cross-document Q&A loop.**


## ğŸ“„ License
MIT License

## ğŸ“¬ Submission Alignment
1. Included: code + setup guide (see Quick Start).
2. Architecture, design decisions & challenges: see respective sections.

