"""
Document chunking utilities for PDF and CSV files.
Handles splitting documents into manageable chunks with metadata.
"""

import re
from typing import List, Dict, Any, Optional
from pathlib import Path
import pandas as pd
import json

class DocumentChunker:
    """
    æ–‡æ¡£åˆ†å—å™¨ï¼Œæ”¯æŒPDFå’ŒCSVæ–‡æ¡£çš„åˆ†å—å¤„ç†
    """
    
    def __init__(self, 
                 chunk_size: int = 1000, 
                 chunk_overlap: int = 200,
                 pdf_page_chunk: bool = True,
                 csv_row_chunk: bool = True):
        """
        åˆå§‹åŒ–åˆ†å—å™¨
        
        Args:
            chunk_size: å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            chunk_overlap: å—é‡å å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            pdf_page_chunk: PDFæ˜¯å¦æŒ‰é¡µé¢åˆ†å—
            csv_row_chunk: CSVæ˜¯å¦æŒ‰è¡Œåˆ†å—
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.pdf_page_chunk = pdf_page_chunk
        self.csv_row_chunk = csv_row_chunk
    
    def chunk_pdf_markdown(self, 
                          markdown_content: str, 
                          document_metadata: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        åˆ†å—PDFçš„Markdownå†…å®¹
        
        Args:
            markdown_content: PDFè½¬æ¢åçš„Markdownå†…å®¹
            document_metadata: æ–‡æ¡£å…ƒæ•°æ®
            
        Returns:
            åˆ†å—åˆ—è¡¨
        """
        chunks = []
        
        if self.pdf_page_chunk:
            # æŒ‰é¡µé¢åˆ†å—ï¼ˆå‡è®¾é¡µé¢ä¹‹é—´æœ‰åˆ†éš”ç¬¦ï¼‰
            pages = self._split_by_pages(markdown_content)
            
            for page_num, page_content in enumerate(pages, 1):
                if page_content.strip():
                    chunk = {
                        "chunk_id": f"{document_metadata['document_id']}_page_{page_num}",
                        "document_id": document_metadata["document_id"],
                        "content": page_content.strip(),
                        "metadata": {
                            "file_type": "pdf",
                            "filename": document_metadata.get("filename", "unknown.pdf"),
                            "page": page_num,
                            "chunk_type": "page",
                            "total_pages": len(pages),
                            "chunk_size": len(page_content)
                        }
                    }
                    chunks.append(chunk)
        else:
            # æŒ‰å›ºå®šå¤§å°åˆ†å—
            chunks = self._split_by_size(
                markdown_content, 
                document_metadata, 
                "pdf"
            )
        
        print(f"ğŸ“„ PDFåˆ†å—å®Œæˆ: {len(chunks)} ä¸ªå—")
        return chunks
    
    def chunk_csv_markdown(self, 
                          markdown_content: str, 
                          document_metadata: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        åˆ†å—CSVçš„Markdownå†…å®¹
        
        Args:
            markdown_content: CSVè½¬æ¢åçš„Markdownå†…å®¹
            document_metadata: æ–‡æ¡£å…ƒæ•°æ®
            
        Returns:
            åˆ†å—åˆ—è¡¨
        """
        chunks = []
        
        if self.csv_row_chunk:
            # æŒ‰è¡Œåˆ†å—
            chunks = self._split_csv_by_rows(markdown_content, document_metadata)
        else:
            # æŒ‰å›ºå®šå¤§å°åˆ†å—
            chunks = self._split_by_size(
                markdown_content, 
                document_metadata, 
                "csv"
            )
        
        print(f"ğŸ“Š CSVåˆ†å—å®Œæˆ: {len(chunks)} ä¸ªå—")
        return chunks
    
    def _split_by_pages(self, markdown_content: str) -> List[str]:
        """
        æŒ‰é¡µé¢åˆ†å‰²Markdownå†…å®¹
        
        Args:
            markdown_content: Markdownå†…å®¹
            
        Returns:
            é¡µé¢åˆ—è¡¨
        """
        # å°è¯•å¤šç§é¡µé¢åˆ†éš”ç¬¦
        page_separators = [
            r'\n---\n',  # æ ‡å‡†é¡µé¢åˆ†éš”ç¬¦
            r'\n\n---\n\n',  # å¸¦ç©ºè¡Œçš„åˆ†éš”ç¬¦
            r'\n# Page \d+\n',  # é¡µé¢æ ‡é¢˜
            r'\n## Page \d+\n',  # é¡µé¢å­æ ‡é¢˜
        ]
        
        pages = [markdown_content]
        
        for separator in page_separators:
            new_pages = []
            for page in pages:
                split_pages = re.split(separator, page)
                new_pages.extend(split_pages)
            pages = new_pages
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é¡µé¢åˆ†éš”ç¬¦ï¼Œå°è¯•æŒ‰æ ‡é¢˜åˆ†å‰²
        if len(pages) == 1:
            pages = self._split_by_headers(pages[0])
        
        return pages
    
    def _split_by_headers(self, markdown_content: str) -> List[str]:
        """
        æŒ‰æ ‡é¢˜åˆ†å‰²Markdownå†…å®¹
        
        Args:
            markdown_content: Markdownå†…å®¹
            
        Returns:
            åˆ†å‰²åçš„å†…å®¹åˆ—è¡¨
        """
        # æŒ‰ä¸€çº§æ ‡é¢˜åˆ†å‰²
        sections = re.split(r'\n# ', markdown_content)
        
        # é‡æ–°æ·»åŠ æ ‡é¢˜
        if len(sections) > 1:
            sections[0] = sections[0]  # ç¬¬ä¸€éƒ¨åˆ†å¯èƒ½æ²¡æœ‰æ ‡é¢˜
            for i in range(1, len(sections)):
                sections[i] = '# ' + sections[i]
        
        return sections
    
    def _split_csv_by_rows(self, 
                          markdown_content: str, 
                          document_metadata: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        æŒ‰è¡Œåˆ†å‰²CSVçš„Markdownå†…å®¹
        
        Args:
            markdown_content: CSVçš„Markdownå†…å®¹
            document_metadata: æ–‡æ¡£å…ƒæ•°æ®
            
        Returns:
            åˆ†å—åˆ—è¡¨
        """
        chunks = []
        lines = markdown_content.split('\n')
        
        # æ‰¾åˆ°è¡¨æ ¼è¡Œ
        table_lines = []
        header_line = None
        
        for i, line in enumerate(lines):
            if line.strip().startswith('|') and '|' in line[1:]:
                if header_line is None:
                    header_line = i
                table_lines.append((i, line))
        
        if not table_lines:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¡¨æ ¼ï¼ŒæŒ‰æ™®é€šæ–‡æœ¬å¤„ç†
            return self._split_by_size(markdown_content, document_metadata, "csv")
        
        # æŒ‰è¡Œåˆ†å—
        for i, (line_num, line) in enumerate(table_lines):
            if i == 0:  # è·³è¿‡è¡¨å¤´
                continue
            
            chunk = {
                "chunk_id": f"{document_metadata['document_id']}_row_{i}",
                "document_id": document_metadata["document_id"],
                "content": line.strip(),
                "metadata": {
                    "file_type": "csv",
                    "filename": document_metadata.get("filename", "unknown.csv"),
                    "row": i,
                    "chunk_type": "row",
                    "total_rows": len(table_lines) - 1,  # å‡å»è¡¨å¤´
                    "chunk_size": len(line)
                }
            }
            chunks.append(chunk)
        
        return chunks
    
    def _split_by_size(self, 
                      content: str, 
                      document_metadata: Dict[str, Any], 
                      file_type: str) -> List[Dict[str, Any]]:
        """
        æŒ‰å›ºå®šå¤§å°åˆ†å‰²å†…å®¹
        
        Args:
            content: è¦åˆ†å‰²çš„å†…å®¹
            document_metadata: æ–‡æ¡£å…ƒæ•°æ®
            file_type: æ–‡ä»¶ç±»å‹
            
        Returns:
            åˆ†å—åˆ—è¡¨
        """
        chunks = []
        start = 0
        chunk_num = 0
        
        while start < len(content):
            end = start + self.chunk_size
            
            # å°è¯•åœ¨å¥å­è¾¹ç•Œåˆ†å‰²
            if end < len(content):
                # å‘åæŸ¥æ‰¾å¥å·ã€æ¢è¡Œç¬¦ç­‰åˆ†å‰²ç‚¹
                for i in range(end, max(start + self.chunk_size // 2, end - 100), -1):
                    if content[i] in '.\n!?':
                        end = i + 1
                        break
            
            chunk_content = content[start:end].strip()
            
            if chunk_content:
                chunk = {
                    "chunk_id": f"{document_metadata['document_id']}_chunk_{chunk_num}",
                    "document_id": document_metadata["document_id"],
                    "content": chunk_content,
                    "metadata": {
                        "file_type": file_type,
                        "filename": document_metadata.get("filename", f"unknown.{file_type}"),
                        "chunk_num": chunk_num,
                        "chunk_type": "size_based",
                        "chunk_size": len(chunk_content),
                        "start_pos": start,
                        "end_pos": end
                    }
                }
                chunks.append(chunk)
                chunk_num += 1
            
            # è®¾ç½®ä¸‹ä¸€ä¸ªå—çš„èµ·å§‹ä½ç½®ï¼ˆè€ƒè™‘é‡å ï¼‰
            start = end - self.chunk_overlap
            if start < 0:
                start = end
        
        return chunks
    
    def chunk_document(self, 
                      content: str, 
                      file_type: str, 
                      document_metadata: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        é€šç”¨æ–‡æ¡£åˆ†å—æ¥å£
        
        Args:
            content: æ–‡æ¡£å†…å®¹
            file_type: æ–‡ä»¶ç±»å‹ (pdf/csv)
            document_metadata: æ–‡æ¡£å…ƒæ•°æ®
            
        Returns:
            åˆ†å—åˆ—è¡¨
        """
        if file_type.lower() == "pdf":
            return self.chunk_pdf_markdown(content, document_metadata)
        elif file_type.lower() == "csv":
            return self.chunk_csv_markdown(content, document_metadata)
        else:
            raise ValueError(f"Unsupported file type: {file_type}")
    
    def save_chunks(self, chunks: List[Dict[str, Any]], output_path: str):
        """
        ä¿å­˜åˆ†å—åˆ°æ–‡ä»¶
        
        Args:
            chunks: åˆ†å—åˆ—è¡¨
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(chunks, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ åˆ†å—å·²ä¿å­˜åˆ°: {output_path}")
    
    def load_chunks(self, input_path: str) -> List[Dict[str, Any]]:
        """
        ä»æ–‡ä»¶åŠ è½½åˆ†å—
        
        Args:
            input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
            
        Returns:
            åˆ†å—åˆ—è¡¨
        """
        with open(input_path, 'r', encoding='utf-8') as f:
            chunks = json.load(f)
        
        print(f"ğŸ“‚ ä» {input_path} åŠ è½½äº† {len(chunks)} ä¸ªåˆ†å—")
        return chunks

def test_chunker():
    """æµ‹è¯•åˆ†å—å™¨åŠŸèƒ½"""
    print("ğŸ§ª Testing DocumentChunker...")
    
    # åˆ›å»ºåˆ†å—å™¨
    chunker = DocumentChunker()
    
    # æµ‹è¯•PDFåˆ†å—
    pdf_content = """# Document Title

This is page 1 content.

---

# Page 2

This is page 2 content.

---

# Page 3

This is page 3 content."""
    
    pdf_metadata = {
        "document_id": "test_pdf_001",
        "filename": "test.pdf"
    }
    
    pdf_chunks = chunker.chunk_pdf_markdown(pdf_content, pdf_metadata)
    print(f"âœ… PDFåˆ†å—: {len(pdf_chunks)} ä¸ªå—")
    
    # æµ‹è¯•CSVåˆ†å—
    csv_content = """# test_data.csv

**è¡Œæ•°**: 5
**åˆ—æ•°**: 4

## å®Œæ•´æ•°æ®

| name   |   age | city   |   salary |
|:-------|------:|:-------|---------:|
| å¼ ä¸‰     |    25 | åŒ—äº¬     |    50000 |
| æå››     |    30 | ä¸Šæµ·     |    60000 |
| ç‹äº”     |    28 | å¹¿å·     |    55000 |"""
    
    csv_metadata = {
        "document_id": "test_csv_001", 
        "filename": "test.csv"
    }
    
    csv_chunks = chunker.chunk_csv_markdown(csv_content, csv_metadata)
    print(f"âœ… CSVåˆ†å—: {len(csv_chunks)} ä¸ªå—")
    
    print("ğŸ‰ All tests passed!")

if __name__ == "__main__":
    test_chunker()
