"""
Build and manage vector databases for PDF and CSV documents.
Implements dual database architecture with CLIP embeddings.
"""

import os
import chromadb
from chromadb.config import Settings
from typing import List, Dict, Any, Optional
from pathlib import Path
import json
from datetime import datetime

from .embedder import DocumentEmbedder
from .chunks import DocumentChunker
from .config import config
from ..tools.parser import parse_pdf, parse_csv

class VectorDatabaseManager:
    """
    å‘é‡æ•°æ®åº“ç®¡ç†å™¨ï¼Œæ”¯æŒPDFå’ŒCSVçš„ç‹¬ç«‹å­˜å‚¨
    ä½¿ç”¨CLIPåµŒå…¥æ¨¡å‹è¿›è¡Œæ–‡æœ¬å‘é‡åŒ–
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨"""
        # ä»é…ç½®æ–‡ä»¶è·å–è®¾ç½®
        vector_config = config.get_vector_store_config()
        
        # åˆå§‹åŒ–åµŒå…¥å™¨å’Œåˆ†å—å™¨
        self.embedder = DocumentEmbedder()
        self.chunker = DocumentChunker()
        
        # åˆå§‹åŒ–PDFæ•°æ®åº“å®¢æˆ·ç«¯
        pdf_db_path = vector_config.get("pdf_database", {}).get("persist_directory", "./chroma_db/pdf_db")
        self.pdf_client = chromadb.PersistentClient(
            path=pdf_db_path,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        
        # åˆå§‹åŒ–CSVæ•°æ®åº“å®¢æˆ·ç«¯
        csv_db_path = vector_config.get("csv_database", {}).get("persist_directory", "./chroma_db/csv_db")
        self.csv_client = chromadb.PersistentClient(
            path=csv_db_path,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        
        # åˆ›å»ºé›†åˆ
        self.pdf_collection = self.pdf_client.get_or_create_collection(
            name="pdf_documents",
            metadata={"description": "PDF documents with page-level chunks"}
        )
        
        self.csv_collection = self.csv_client.get_or_create_collection(
            name="csv_documents", 
            metadata={"description": "CSV documents with row-level chunks"}
        )
        
        print("âœ… VectorDatabaseManager initialized with CLIP embeddings")
        print(f"   ğŸ“ PDF Database: {pdf_db_path}")
        print(f"   ğŸ“ CSV Database: {csv_db_path}")
        print(f"   ğŸ§  Embedding Model: CLIP (512 dimensions)")
    
    def add_pdf_document(self, 
                        document_url: str, 
                        document_id: Optional[str] = None,
                        metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """æ·»åŠ PDFæ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“"""
        try:
            print(f"ğŸ“„ Processing PDF document: {document_url}")
            
            # è§£æPDFæ–‡æ¡£
            parse_result = parse_pdf(document_url)
            if "error" in parse_result:
                return {"error": f"Failed to parse PDF: {parse_result['error']}"}
            
            # ç”Ÿæˆæ–‡æ¡£ID
            if not document_id:
                document_id = f"pdf_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            # å‡†å¤‡æ–‡æ¡£å…ƒæ•°æ®
            doc_metadata = {
                "document_id": document_id,
                "filename": os.path.basename(document_url),
                "file_type": "pdf",
                "url": document_url,
                "upload_time": datetime.now().isoformat(),
                **(metadata or {})
            }
            
            # è·å–Markdownå†…å®¹
            if "markdown_content" not in parse_result:
                return {"error": "No markdown content found in parse result"}
            
            markdown_content = parse_result["markdown_content"]
            
            # åˆ†å—
            chunks = self.chunker.chunk_pdf_markdown(markdown_content, doc_metadata)
            
            if not chunks:
                return {"error": "No chunks generated from PDF"}
            
            # ç”ŸæˆåµŒå…¥å‘é‡
            texts = [chunk["content"] for chunk in chunks]
            embeddings = self.embedder.embed(texts)
            
            # å°†åµŒå…¥å‘é‡æ·»åŠ åˆ°å—ä¸­
            chunks_with_embeddings = []
            for i, chunk in enumerate(chunks):
                chunk["embedding"] = embeddings[i]
                chunks_with_embeddings.append(chunk)
            
            # å‡†å¤‡ChromaDBæ•°æ®
            ids = [chunk["chunk_id"] for chunk in chunks_with_embeddings]
            documents = [chunk["content"] for chunk in chunks_with_embeddings]
            embeddings = [chunk["embedding"] for chunk in chunks_with_embeddings]
            metadatas = [chunk["metadata"] for chunk in chunks_with_embeddings]
            
            # æ·»åŠ åˆ°PDFé›†åˆ
            self.pdf_collection.add(
                ids=ids,
                documents=documents,
                embeddings=embeddings,
                metadatas=metadatas
            )
            
            result = {
                "status": "success",
                "document_id": document_id,
                "database": "pdf_db",
                "collection": "pdf_documents",
                "chunks_added": len(chunks),
                "metadata": doc_metadata
            }
            
            print(f"âœ… PDF document added to pdf_db: {len(chunks)} chunks")
            return result
            
        except Exception as e:
            return {"error": f"Error adding PDF document: {str(e)}"}
    
    def add_csv_document(self, 
                        file_path: str, 
                        document_id: Optional[str] = None,
                        metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """æ·»åŠ CSVæ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“"""
        try:
            print(f"ğŸ“Š Processing CSV document: {file_path}")
            
            # è§£æCSVæ–‡æ¡£
            parse_result = parse_csv(file_path)
            if "error" in parse_result:
                return {"error": f"Failed to parse CSV: {parse_result['error']}"}
            
            # ç”Ÿæˆæ–‡æ¡£ID
            if not document_id:
                document_id = f"csv_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            # å‡†å¤‡æ–‡æ¡£å…ƒæ•°æ®
            doc_metadata = {
                "document_id": document_id,
                "filename": os.path.basename(file_path),
                "file_type": "csv",
                "file_path": file_path,
                "upload_time": datetime.now().isoformat(),
                "rows": parse_result.get("rows", 0),
                "columns": parse_result.get("columns", 0),
                **(metadata or {})
            }
            
            # è·å–Markdownå†…å®¹
            if "markdown_content" not in parse_result:
                return {"error": "No markdown content found in parse result"}
            
            markdown_content = parse_result["markdown_content"]
            
            # åˆ†å—
            chunks = self.chunker.chunk_csv_markdown(markdown_content, doc_metadata)
            
            if not chunks:
                return {"error": "No chunks generated from CSV"}
            
            # ç”ŸæˆåµŒå…¥å‘é‡
            texts = [chunk["content"] for chunk in chunks]
            embeddings = self.embedder.embed(texts)
            
            # å°†åµŒå…¥å‘é‡æ·»åŠ åˆ°å—ä¸­
            chunks_with_embeddings = []
            for i, chunk in enumerate(chunks):
                chunk["embedding"] = embeddings[i]
                chunks_with_embeddings.append(chunk)
            
            # å‡†å¤‡ChromaDBæ•°æ®
            ids = [chunk["chunk_id"] for chunk in chunks_with_embeddings]
            documents = [chunk["content"] for chunk in chunks_with_embeddings]
            embeddings = [chunk["embedding"] for chunk in chunks_with_embeddings]
            metadatas = [chunk["metadata"] for chunk in chunks_with_embeddings]
            
            # æ·»åŠ åˆ°CSVé›†åˆ
            self.csv_collection.add(
                ids=ids,
                documents=documents,
                embeddings=embeddings,
                metadatas=metadatas
            )
            
            result = {
                "status": "success",
                "document_id": document_id,
                "database": "csv_db",
                "collection": "csv_documents",
                "chunks_added": len(chunks),
                "metadata": doc_metadata
            }
            
            print(f"âœ… CSV document added to csv_db: {len(chunks)} chunks")
            return result
            
        except Exception as e:
            return {"error": f"Error adding CSV document: {str(e)}"}
    
    def search_pdf_documents(self, 
                           query: str, 
                           top_k: int = 5,
                           filters: Optional[Dict] = None) -> Dict[str, Any]:
        """åœ¨PDFæ–‡æ¡£ä¸­æœç´¢"""
        try:
            # ä½¿ç”¨CLIPåµŒå…¥å™¨ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = self.embedder.embed(query)
            
            results = self.pdf_collection.query(
                query_embeddings=[query_embedding],
                n_results=top_k,
                where=filters,
                include=['documents', 'metadatas', 'distances']
            )
            
            return {
                "status": "success",
                "database": "pdf_db",
                "collection": "pdf_documents",
                "query": query,
                "results": results
            }
            
        except Exception as e:
            return {"error": f"Error searching PDF documents: {str(e)}"}
    
    def search_csv_documents(self, 
                           query: str, 
                           top_k: int = 5,
                           filters: Optional[Dict] = None) -> Dict[str, Any]:
        """åœ¨CSVæ–‡æ¡£ä¸­æœç´¢"""
        try:
            # ä½¿ç”¨CLIPåµŒå…¥å™¨ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = self.embedder.embed(query)
            
            results = self.csv_collection.query(
                query_embeddings=[query_embedding],
                n_results=top_k,
                where=filters,
                include=['documents', 'metadatas', 'distances']
            )
            
            return {
                "status": "success",
                "database": "csv_db",
                "collection": "csv_documents",
                "query": query,
                "results": results
            }
            
        except Exception as e:
            return {"error": f"Error searching CSV documents: {str(e)}"}
    
    def search_all_documents(self, 
                           query: str, 
                           top_k: int = 5,
                           file_type: Optional[str] = None) -> Dict[str, Any]:
        """åœ¨æ‰€æœ‰æ–‡æ¡£ä¸­æœç´¢"""
        results = {
            "status": "success",
            "query": query,
            "pdf_results": None,
            "csv_results": None
        }
        
        if file_type is None or file_type == "pdf":
            pdf_results = self.search_pdf_documents(query, top_k)
            if "error" not in pdf_results:
                results["pdf_results"] = pdf_results
        
        if file_type is None or file_type == "csv":
            csv_results = self.search_csv_documents(query, top_k)
            if "error" not in csv_results:
                results["csv_results"] = csv_results
        
        return results
    
    def get_collection_stats(self) -> Dict[str, Any]:
        """è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯"""
        pdf_count = self.pdf_collection.count()
        csv_count = self.csv_collection.count()
        
        vector_config = config.get_vector_store_config()
        
        return {
            "pdf_database": {
                "path": vector_config.get("pdf_database", {}).get("persist_directory", "./chroma_db/pdf_db"),
                "collection_name": "pdf_documents",
                "total_chunks": pdf_count,
                "description": "PDF documents with page-level chunks"
            },
            "csv_database": {
                "path": vector_config.get("csv_database", {}).get("persist_directory", "./chroma_db/csv_db"),
                "collection_name": "csv_documents", 
                "total_chunks": csv_count,
                "description": "CSV documents with row-level chunks"
            },
            "embedding_model": "CLIP (512 dimensions)",
            "total_chunks": pdf_count + csv_count
        }
    
    def reset_databases(self):
        """é‡ç½®æ‰€æœ‰æ•°æ®åº“"""
        # é‡ç½®PDFæ•°æ®åº“
        self.pdf_client.delete_collection("pdf_documents")
        self.pdf_collection = self.pdf_client.get_or_create_collection(
            name="pdf_documents",
            metadata={"description": "PDF documents with page-level chunks"}
        )
        
        # é‡ç½®CSVæ•°æ®åº“
        self.csv_client.delete_collection("csv_documents")
        self.csv_collection = self.csv_client.get_or_create_collection(
            name="csv_documents",
            metadata={"description": "CSV documents with row-level chunks"}
        )
        
        vector_config = config.get_vector_store_config()
        
        print("ğŸ”„ Databases reset successfully")
        print(f"   ğŸ“ PDF Database: {vector_config.get('pdf_database', {}).get('persist_directory', './chroma_db/pdf_db')}")
        print(f"   ğŸ“ CSV Database: {vector_config.get('csv_database', {}).get('persist_directory', './chroma_db/csv_db')}")

def test_database_manager():
    """æµ‹è¯•æ•°æ®åº“ç®¡ç†å™¨"""
    print("ğŸ§ª Testing VectorDatabaseManager with CLIP embeddings...")
    
    try:
        # åˆ›å»ºæ•°æ®åº“ç®¡ç†å™¨
        db_manager = VectorDatabaseManager()
        
        # æµ‹è¯•CSVæ–‡æ¡£æ·»åŠ 
        csv_file = "test_data.csv"
        if os.path.exists(csv_file):
            result = db_manager.add_csv_document(csv_file)
            if "error" not in result:
                print(f"âœ… CSV document added: {result['chunks_added']} chunks")
            else:
                print(f"âŒ CSV document failed: {result['error']}")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = db_manager.get_collection_stats()
        print(f"ğŸ“Š Database stats: {stats}")
        
        # æµ‹è¯•æœç´¢
        if stats["csv_database"]["total_chunks"] > 0:
            search_results = db_manager.search_csv_documents("å¼ ä¸‰")
            print(f"ğŸ” Search test: {len(search_results.get('results', {}).get('documents', [[]])[0])} results")
        
        print("ğŸ‰ Database manager test completed!")
        
    except Exception as e:
        print(f"âŒ Test failed: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_database_manager()