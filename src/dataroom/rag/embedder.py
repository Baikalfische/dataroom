"""
Text Embedding model using CLIP for document vectorization.
Uses CLIP text encoder for text embeddings only.
"""

import os
import torch
from transformers import CLIPProcessor, CLIPModel
from typing import List, Union, Optional
from dotenv import load_dotenv

from .config import config

# Load environment variables
load_dotenv()

class DocumentEmbedder:
    """
    æ–‡æœ¬æ–‡æ¡£åµŒå…¥å™¨ï¼Œä½¿ç”¨CLIPæ¨¡å‹çš„æ–‡æœ¬ç¼–ç å™¨
    """
    
    def __init__(self, model_name: Optional[str] = None, device: Optional[str] = None):
        """
        åˆå§‹åŒ–CLIPæ–‡æœ¬åµŒå…¥å™¨
        
        Args:
            model_name: CLIPæ¨¡å‹åç§°ï¼ŒNoneåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®
            device: è¿è¡Œè®¾å¤‡ï¼ŒNoneåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®
        """
        # ä»é…ç½®æ–‡ä»¶è·å–è®¾ç½®
        embed_config = config.get_embedding_config()
        self.model_name = model_name or embed_config.get("name", "openai/clip-vit-base-patch32")
        
        # è®¾å¤‡è®¾ç½®
        if device:
            self.device = device
        elif embed_config.get("device") == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = embed_config.get("device", "cpu")
        
        print(f"ğŸš€ Initializing CLIP Text Embedder on device: {self.device}")
        
        # è®¾ç½®æ¨¡å‹ç¼“å­˜ç›®å½•
        cache_dir = './model-weights/huggingface'
        clip_model_path = os.path.join(cache_dir, 'clip')
        clip_processor_path = os.path.join(cache_dir, 'clip_processor')
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        os.makedirs(clip_model_path, exist_ok=True)
        os.makedirs(clip_processor_path, exist_ok=True)
        
        # åŠ è½½CLIPæ¨¡å‹å’Œå¤„ç†å™¨
        self.model = CLIPModel.from_pretrained(
            self.model_name, 
            cache_dir=clip_model_path
        ).to(self.device)
        
        self.processor = CLIPProcessor.from_pretrained(
            self.model_name, 
            cache_dir=clip_processor_path
        )
        
        print(f"âœ… CLIP model loaded successfully from {self.model_name}")
    
    def embed(self, texts: Union[str, List[str]]) -> Union[List[float], List[List[float]]]:
        """
        åµŒå…¥æ–¹æ³•ï¼Œå¤„ç†å•ä¸ªæ–‡æœ¬æˆ–å¤šä¸ªæ–‡æœ¬
        
        Args:
            texts: å•ä¸ªæ–‡æœ¬å­—ç¬¦ä¸²æˆ–æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            å•ä¸ªæ–‡æœ¬è¿”å›åµŒå…¥å‘é‡ï¼Œå¤šä¸ªæ–‡æœ¬è¿”å›åµŒå…¥å‘é‡åˆ—è¡¨
        """
        # ç»Ÿä¸€å¤„ç†ï¼šå•ä¸ªæ–‡æœ¬è½¬ä¸ºåˆ—è¡¨
        if isinstance(texts, str):
            texts = [texts]
            return_single = True
        else:
            return_single = False
        
        if not texts:
            return [] if not return_single else []
        
        try:
            with torch.no_grad():
                text_inputs = self.processor(
                    text=texts, 
                    return_tensors="pt", 
                    padding=True, 
                    truncation=True
                ).to(self.device)
                
                text_emb = self.model.get_text_features(**text_inputs)
                # å½’ä¸€åŒ–
                text_emb = text_emb / text_emb.norm(p=2, dim=-1, keepdim=True)
                
                # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
                embeddings = text_emb.cpu().numpy().tolist()
                
                # å¦‚æœè¾“å…¥æ˜¯å•ä¸ªæ–‡æœ¬ï¼Œè¿”å›å•ä¸ªå‘é‡
                if return_single:
                    return embeddings[0]
                else:
                    return embeddings
                
        except Exception as e:
            print(f"âŒ Error embedding texts: {str(e)}")
            raise